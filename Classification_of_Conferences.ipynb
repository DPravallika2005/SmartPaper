{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DPravallika2005/SmartPaper/blob/main/Classification_of_Conferences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz0tgKMHIVuP",
        "outputId": "02b1d2c2-8352-4878-8c57-58c502f7166b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "pip install torch transformers pdfplumber sklearn nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGJbtKAMKHUq",
        "outputId": "fde0afde-8e81-40e4-a339-0ff0b6e2f65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Using cached pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02128MhZ6iUi",
        "outputId": "97f868df-2b42-4c6c-ac09-b3da6411310a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "The folder exists.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, Loss: 0.8153330683708191\n",
            "Validation Accuracy: 0.3333333333333333\n",
            "Epoch 2/3, Loss: 0.7370777428150177\n",
            "Validation Accuracy: 0.6666666666666666\n",
            "Epoch 3/3, Loss: 0.6673690378665924\n",
            "Validation Accuracy: 0.6666666666666666\n",
            "Model saved as bert_classifier.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "import pdfplumber\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to folder in Google Drive\n",
        "folder_path = '/content/drive/My Drive/Reference_Papers'\n",
        "\n",
        "# Check if the folder exists\n",
        "if os.path.exists(folder_path):\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(\"The folder exists.\")\n",
        "    else:\n",
        "        print(\"The path exists, but it is not a folder.\")\n",
        "else:\n",
        "    print(\"The folder does not exist.\")\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# === Step 1: Preprocessing ===\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text: lowercase, remove stop words, punctuation.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# === Step 2: Load Data from PDF Files ===\n",
        "\n",
        "def load_data_from_pdf(folder_path, labels_dict):\n",
        "    \"\"\"Load data directly from PDF files.\"\"\"\n",
        "    texts, labels = [], []\n",
        "\n",
        "    for file_name, label in labels_dict.items():\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"File {file_name} does not exist in the folder.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                text = ''.join(page.extract_text() for page in pdf.pages)\n",
        "                texts.append(preprocess_text(text.strip()))\n",
        "                labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_name}: {e}\")\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "# === Step 3: Define Dataset and Model ===\n",
        "\n",
        "class ResearchPapersDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, 2)  # 2 classes: Publishable/Non-Publishable\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]  # CLS token representation\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "\n",
        "# === Step 4: Train the Model ===\n",
        "\n",
        "def train_model(train_loader, val_loader, model, optimizer, criterion, device, epochs=3):\n",
        "    \"\"\"Train the BERT classifier.\"\"\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        print(\"Validation Accuracy:\", accuracy_score(val_labels, val_preds))\n",
        "\n",
        "    return model\n",
        "\n",
        "# === Step 5: Inference ===\n",
        "\n",
        "def predict(model, test_loader, device):\n",
        "    \"\"\"Predict using the trained model.\"\"\"\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    return preds\n",
        "\n",
        "# === Main Workflow ===\n",
        "\n",
        "def main():\n",
        "    # Path to folder containing PDF files in Google Drive\n",
        "    folder_path = '/content/drive/My Drive/Reference_Papers'\n",
        "\n",
        "    # Define labels for each file\n",
        "    labels_dict = {\n",
        "        \"R001.pdf\": 0,  # Publishable\n",
        "        \"R002.pdf\": 0,  # Non-Publishable\n",
        "        \"R003.pdf\": 0,  # Publishable\n",
        "        \"R004.pdf\": 0,\n",
        "        \"R005.pdf\": 0,  # Publishable\n",
        "        \"R006.pdf\": 1,\n",
        "        \"R007.pdf\": 1,  # Publishable\n",
        "        \"R008.pdf\": 1,\n",
        "        \"R009.pdf\": 1,  # Publishable\n",
        "        \"R010.pdf\": 1,\n",
        "        \"R011.pdf\": 1,  # Publishable\n",
        "        \"R012.pdf\": 1,\n",
        "        \"R013.pdf\": 1,  # Publishable\n",
        "        \"R014.pdf\": 1,\n",
        "        \"R015.pdf\": 1,\n",
        "        # Add more files as needed\n",
        "    }\n",
        "\n",
        "    # Load and preprocess data directly from PDF files in Google Drive\n",
        "    texts, labels = load_data_from_pdf(folder_path, labels_dict)\n",
        "\n",
        "    if not texts:\n",
        "        print(\"No data found. Please check the folder path and files.\")\n",
        "        return\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        texts, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize tokenizer and datasets\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    max_length = 512\n",
        "\n",
        "    train_dataset = ResearchPapersDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "    val_dataset = ResearchPapersDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "    # Initialize model, optimizer, and loss function\n",
        "    model = BERTClassifier()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Train the model\n",
        "    model = train_model(train_loader, val_loader, model, optimizer, criterion, device, epochs=3)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"bert_classifier.pth\")\n",
        "    print(\"Model saved as bert_classifier.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXfrPSM6Nwll",
        "outputId": "c4fc10ee-bc66-40ea-c449-4e452b3ba734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "The folder exists.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, Loss: 0.6551213264465332\n",
            "Validation Accuracy: 0.6666666666666666\n",
            "Epoch 2/3, Loss: 0.6568460166454315\n",
            "Validation Accuracy: 0.6666666666666666\n",
            "Epoch 3/3, Loss: 0.7432746887207031\n",
            "Validation Accuracy: 0.6666666666666666\n",
            "Model saved as bert_classifier.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-d8ebd0f18092>:273: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"bert_classifier.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The file is classified as: Publishable\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "import pdfplumber\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to folder in Google Drive\n",
        "folder_path = '/content/drive/My Drive/Reference_Papers'\n",
        "\n",
        "# Check if the folder exists\n",
        "if os.path.exists(folder_path):\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(\"The folder exists.\")\n",
        "    else:\n",
        "        print(\"The path exists, but it is not a folder.\")\n",
        "else:\n",
        "    print(\"The folder does not exist.\")\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# === Step 1: Preprocessing ===\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text: lowercase, remove stop words, punctuation.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# === Step 2: Load Data from PDF Files ===\n",
        "\n",
        "def load_data_from_pdf(folder_path, labels_dict):\n",
        "    \"\"\"Load data directly from PDF files.\"\"\"\n",
        "    texts, labels = [], []\n",
        "\n",
        "    for file_name, label in labels_dict.items():\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"File {file_name} does not exist in the folder.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                text = ''.join(page.extract_text() for page in pdf.pages)\n",
        "                texts.append(preprocess_text(text.strip()))\n",
        "                labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_name}: {e}\")\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "# === Step 3: Define Dataset and Model ===\n",
        "\n",
        "class ResearchPapersDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, 2)  # 2 classes: Publishable/Non-Publishable\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]  # CLS token representation\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "\n",
        "# === Step 4: Train the Model ===\n",
        "\n",
        "def train_model(train_loader, val_loader, model, optimizer, criterion, device, epochs=3):\n",
        "    \"\"\"Train the BERT classifier.\"\"\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        print(\"Validation Accuracy:\", accuracy_score(val_labels, val_preds))\n",
        "\n",
        "    return model\n",
        "\n",
        "# === Step 5: Inference ===\n",
        "\n",
        "def predict(model, test_loader, device):\n",
        "    \"\"\"Predict using the trained model.\"\"\"\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    return preds\n",
        "\n",
        "# === Step 6: Test the Model on New PDF ===\n",
        "\n",
        "def test_new_file(file_path, model, tokenizer, device, max_length=512):\n",
        "    \"\"\"Test the trained model on a new PDF file.\"\"\"\n",
        "    # Extract and preprocess text from the new PDF\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            text = ''.join(page.extract_text() for page in pdf.pages)\n",
        "            processed_text = preprocess_text(text.strip())\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    # Tokenize the text\n",
        "    encoding = tokenizer(\n",
        "        processed_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # Make a prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        prediction = torch.argmax(outputs, dim=1).item()  # Get the predicted class (0 or 1)\n",
        "\n",
        "    # Return the prediction\n",
        "    return \"Publishable\" if prediction == 0 else \"Non-Publishable\"\n",
        "\n",
        "\n",
        "# === Main Workflow ===\n",
        "\n",
        "def main():\n",
        "    # Path to folder containing PDF files in Google Drive\n",
        "    folder_path = '/content/drive/My Drive/Reference_Papers'\n",
        "\n",
        "    # Define labels for each file\n",
        "    labels_dict = {\n",
        "        \"R001.pdf\": 0,  # Publishable\n",
        "        \"R002.pdf\": 0,  # Non-Publishable\n",
        "        \"R003.pdf\": 0,  # Publishable\n",
        "        \"R004.pdf\": 0,\n",
        "        \"R005.pdf\": 0,  # Publishable\n",
        "        \"R006.pdf\": 1,\n",
        "        \"R007.pdf\": 1,  # Publishable\n",
        "        \"R008.pdf\": 1,\n",
        "        \"R009.pdf\": 1,  # Publishable\n",
        "        \"R010.pdf\": 1,\n",
        "        \"R011.pdf\": 1,  # Publishable\n",
        "        \"R012.pdf\": 1,\n",
        "        \"R013.pdf\": 1,  # Publishable\n",
        "        \"R014.pdf\": 1,\n",
        "        \"R015.pdf\": 1,\n",
        "        # Add more files as needed\n",
        "    }\n",
        "\n",
        "    # Load and preprocess data directly from PDF files in Google Drive\n",
        "    texts, labels = load_data_from_pdf(folder_path, labels_dict)\n",
        "\n",
        "    if not texts:\n",
        "        print(\"No data found. Please check the folder path and files.\")\n",
        "        return\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        texts, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize tokenizer and datasets\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    max_length = 512\n",
        "\n",
        "    train_dataset = ResearchPapersDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "    val_dataset = ResearchPapersDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "    # Initialize model, optimizer, and loss function\n",
        "    model = BERTClassifier()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Train the model\n",
        "    model = train_model(train_loader, val_loader, model, optimizer, criterion, device, epochs=3)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"bert_classifier.pth\")\n",
        "    print(\"Model saved as bert_classifier.pth\")\n",
        "\n",
        "    # Load the trained model\n",
        "    model = BERTClassifier()\n",
        "    model.load_state_dict(torch.load(\"bert_classifier.pth\"))\n",
        "    model.to(device)\n",
        "\n",
        "    # Test a new file\n",
        "    new_file_path = '/content/R007.pdf'  # Specify the new file here\n",
        "    result = test_new_file(new_file_path, model, tokenizer, device)\n",
        "    print(f\"The file is classified as: {result}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imbalanced-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c94724PBBc7e",
        "outputId": "52ad6007-b4a1-48de-f47e-da4a8980e642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.6.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2\n"
      ],
      "metadata": {
        "id": "KYBpg6Cy45IB",
        "outputId": "40ffb334-1f00-4dd2-e186-5335480ca9ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-gpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nERLpyWnS9I",
        "outputId": "e3c6ddce-20fa-4338-f9b0-7b997a90ce2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORKING CODE**"
      ],
      "metadata": {
        "id": "sVth9EvV7RFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import faiss\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "\n",
        "# Step 1: Initialize the Embedding Model and Text Generation Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "embedding_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "text_generation_model = pipeline(\"text-generation\", model=\"gpt2\")  # GPT-2 for rationale generation\n",
        "\n",
        "# Step 2: Function to Extract Text from PDFs\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and preprocesses the text extracted from a PDF.\"\"\"\n",
        "    text = \" \".join(text.split())  # Remove multiple spaces and newlines\n",
        "    text = re.sub(r\"(?i)References.*\", \"\", text)  # Remove references section\n",
        "    text = re.sub(r\"(Figure|Table) \\d+.*\", \"\", text)  # Remove figure/table captions\n",
        "    return text\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts and preprocesses text from a given PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return preprocess_text(text.strip())\n",
        "\n",
        "# Step 3: Text Chunking Function (Handle Large Texts)\n",
        "def chunk_text(text, chunk_size=512):\n",
        "    \"\"\"Split text into chunks of a given token size.\"\"\"\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "def create_embedding(text):\n",
        "    \"\"\"Generate embedding for the given text, handling chunking and truncation.\"\"\"\n",
        "    chunks = chunk_text(text, chunk_size=512)\n",
        "    chunk_vectors = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        # Tokenize and truncate the chunk\n",
        "        tokens = tokenizer(chunk, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "            embeddings = embedding_model(**tokens).last_hidden_state.mean(dim=1).detach().numpy()\n",
        "        chunk_vectors.append(embeddings)\n",
        "\n",
        "    # Ensure final vector is 2D\n",
        "    return np.mean(chunk_vectors, axis=0).reshape(1, -1)\n",
        "\n",
        "# Step 4: Custom VectorStore Implementation with FAISS for Similarity Search\n",
        "class SimpleVectorStore:\n",
        "    def __init__(self, dimension=768): # Corrected the method name to __init__\n",
        "        self.vectors = []  # List to store vectors\n",
        "        self.metadata = []  # List to store metadata\n",
        "        self.index = faiss.IndexFlatL2(dimension)  # 768-dim vectors for SciBERT\n",
        "\n",
        "    def add_vector(self, key, vector, metadata=None):\n",
        "        \"\"\"Add a vector to the FAISS index.\"\"\"\n",
        "        # Ensure vector is 2D\n",
        "        if len(vector.shape) == 1:\n",
        "            vector = vector.reshape(1, -1)  # Reshape to (1, dimension)\n",
        "        self.vectors.append({\"key\": key, \"vector\": vector, \"metadata\": metadata})\n",
        "        self.index.add(vector.astype(np.float32))  # Add to FAISS index\n",
        "\n",
        "    def search(self, query_vector=None, key=None, top_k=10):\n",
        "        \"\"\"Search for the most similar vectors.\"\"\"\n",
        "        if key:\n",
        "            return [v for v in self.vectors if v[\"key\"] == key]\n",
        "        elif query_vector is not None:\n",
        "            # Reshape the query_vector if necessary to ensure it's 2D\n",
        "            query_vector = query_vector.reshape(1, -1) if query_vector.ndim == 1 else query_vector\n",
        "            distances, indices = self.index.search(query_vector.astype(np.float32), top_k)\n",
        "            results = [\n",
        "                {\"key\": self.vectors[idx][\"key\"], \"score\": 1 / (1 + distances[0][i]), \"metadata\": self.vectors[idx][\"metadata\"]}\n",
        "                for i, idx in enumerate(indices[0])\n",
        "            ]\n",
        "            return results\n",
        "        return []\n",
        "# Initialize the custom vector store\n",
        "vector_store = SimpleVectorStore()\n",
        "\n",
        "# Step 5: Store Conference Papers in the VectorStore\n",
        "def store_conference_papers(conference_papers):\n",
        "    \"\"\"Store all conference papers in the VectorStore.\"\"\"\n",
        "    for conference, pdf_paths in conference_papers.items():\n",
        "        for pdf_path in pdf_paths:\n",
        "            paper_text = extract_text_from_pdf(pdf_path)\n",
        "            vector = create_embedding(paper_text)\n",
        "            vector_store.add_vector(key=conference, vector=vector, metadata={\"text\": paper_text})\n",
        "    print(\"All conference papers have been stored in the VectorStore.\")\n",
        "\n",
        "# Step 6: Define Conference Keywords\n",
        "conference_keywords = {\n",
        "    \"CVPR\": [\"object detection\", \"image segmentation\", \"computer vision tasks\", \"convolutional networks\"],\n",
        "    \"EMNLP\": [\"language models\", \"semantic parsing\", \"text classification\", \"token embeddings\"],\n",
        "    \"KDD\": [\"data clustering\", \"knowledge discovery\", \"graph mining\", \"recommendation systems\"],\n",
        "    \"NeurIPS\": [\"stochastic gradient descent\", \"adversarial training\", \"multi-agent systems\"],\n",
        "    \"TMLR\": [\"optimization techniques\", \"mathematical proofs\", \"theoretical guarantees\", \"learning rates\"]\n",
        "}\n",
        "\n",
        "\n",
        "def compute_keyword_overlap(text, conference):\n",
        "    \"\"\"Compute overlap between text and conference keywords.\"\"\"\n",
        "    keywords = conference_keywords[conference]\n",
        "    overlap = sum(1 for word in keywords if word in text.lower())\n",
        "    return overlap\n",
        "\n",
        "# Step 7: Match a New Paper to a Conference\n",
        "def match_to_conference(new_pdf_path):\n",
        "    \"\"\"Match a new paper to the most relevant conference.\"\"\"\n",
        "    new_text = extract_text_from_pdf(new_pdf_path)\n",
        "    new_vector = create_embedding(new_text)\n",
        "\n",
        "    results = vector_store.search(query_vector=new_vector, top_k=10)\n",
        "\n",
        "    similarity_sums = defaultdict(float)\n",
        "    for result in results:\n",
        "        conference = result['key']\n",
        "        similarity = result['score']\n",
        "        similarity_sums[conference] += similarity\n",
        "\n",
        "    for conference in similarity_sums:\n",
        "        overlap_score = compute_keyword_overlap(new_text, conference)\n",
        "        similarity_sums[conference] += overlap_score\n",
        "\n",
        "    best_conference = max(similarity_sums, key=similarity_sums.get)\n",
        "    best_score = similarity_sums[best_conference]\n",
        "\n",
        "    rationale = generate_rationale(best_conference, new_text)\n",
        "    return best_conference, best_score, rationale\n",
        "\n",
        "# Step 8: Generate Rationale for the Matched Conference\n",
        "def generate_rationale(conference, new_paper_text):\n",
        "    \"\"\"Generate a rationale explaining why the paper matches the selected conference.\"\"\"\n",
        "    conference_papers = []\n",
        "    for result in vector_store.search(query_vector=None, key=conference, top_k=10):\n",
        "        conference_papers.append(result['metadata']['text'])\n",
        "\n",
        "    conference_text = \" \".join(conference_papers)\n",
        "\n",
        "    input_text = f\"The new paper's content is as follows: {new_paper_text[:500]}... The conference {conference} focuses on {conference_text[:500]}...\"\n",
        "\n",
        "    rationale = text_generation_model(input_text, max_new_tokens=200, num_return_sequences=1)\n",
        "    return rationale[0]['generated_text']\n",
        "\n",
        "# Step 9: Define File Paths for Conference Papers\n",
        "conference_papers = {\n",
        "    \"CVPR\": [\"/content/R006.pdf\", \"/content/R007.pdf\", \"/content/cvpr7.pdf\" , \"/content/cvpr6.pdf\", \"/content/cvpr5.pdf\"],\n",
        "    \"EMNLP\": [\"/content/R008.pdf\", \"/content/R009.pdf\", \"/content/emnlp5.pdf\", \"/content/emnlp6.pdf\", \"/content/emnlp7.pdf\"],\n",
        "    \"KDD\": [\"/content/R010.pdf\", \"/content/R011.pdf\", \"/content/kdd6.pdf\", \"/content/kdd7.pdf\", \"/content/kdd5.pdf\"],\n",
        "    \"NeurIPS\": [\"/content/R012.pdf\", \"/content/R013.pdf\", \"/content/neurlps7.pdf\", \"/content/neurlps5.pdf\", \"/content/neurlps6.pdf\"],\n",
        "    \"TMLR\": [\"/content/R014.pdf\", \"/content/R015.pdf\", \"/content/tmlr7.pdf\", \"/content/tmlr5.pdf\", \"/content/tmlr6.pdf\"]\n",
        "}\n",
        "\n",
        "# Step 10: Store the Papers in the VectorStore\n",
        "store_conference_papers(conference_papers)\n",
        "\n",
        "# Step 11: Match a New Paper to a Conference and Generate Rationale\n",
        "new_paper_path = \"/content/P104.pdf\"\n",
        "matched_conference, total_score, rationale = match_to_conference(new_paper_path)\n",
        "\n",
        "print(f\"The new paper is most similar to Conference {matched_conference} with a total similarity score of {total_score:.2f}\")\n",
        "print(f\"Rationale for matching the paper to Conference {matched_conference}: {rationale}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHackq3GkcO5",
        "outputId": "6ec2c524-099a-4db8-8bb6-d200dc28d73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All conference papers have been stored in the VectorStore.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new paper is most similar to Conference EMNLP with a total similarity score of 1.11\n",
            "Rationale for matching the paper to Conference EMNLP: The new paper's content is as follows: Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference Abstract While large pre-trained language models are powerful, their predictions often lack logical consistency across test inputs. For example, a state-of-the-art Macaw question-answering (QA) model answers Yes to Is a sparrow a bird? and Does a bird have feet? but answers No to Does a sparrow have feet?. To address this failure mode, we propose a framework, Consistency Correction throug... The conference EMNLP focuses on Advanced techniques for through and contextually Interpreting Noun-Noun Compounds Abstract This study examines the effectiveness of transfer learning and multi-task learning in the context of a complex semantic classification problem: understanding the meaning of noun-noun compounds. Through a series of detailed experiments and an in-depth analysis of errors, we demonstrate that employing transfer learning by initializing parameters and multi-task learning through parameter sharing enables a neu... The paper's content is as follows: Explanding the Effects of Multivariate and Randomized Interpreter Inference and Selection for a Diverse Memory Task Proportional Memory Strategies Underpassing Memory Sequences and the Effect of Multivariate Random-Skewed Random-Skewed Memory on Sentence Sequences The present paper presents two papers containing significant results on an algorithmically applied memory model.\n",
            "\n",
            "Introduction To understand how automatic inference works we must first understand its implications for human cognition. We argue that our understanding of memory is largely a matter of computation and that all computational models will fail if given insufficient training. An alternative approach to mapping memory inference is to focus on the use of an underlying machine learning method rather than of algorithms. In this paper, we argue for methods of training multiple memory models that can generate large-scale data sets on a large variety of topics, which we call memory models. In addition, we argue in favor of using models designed by non-instant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORKING CODE 1**"
      ],
      "metadata": {
        "id": "AL2TIrbfxux7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import faiss\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "\n",
        "# Step 1: Initialize the Embedding Model and Text Generation Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "embedding_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "text_generation_model = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Step 2: Function to Extract Text from PDFs\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and preprocesses the text extracted from a PDF.\"\"\"\n",
        "    text = \" \".join(text.split())  # Remove multiple spaces and newlines\n",
        "    text = re.sub(r\"(?i)References.*\", \"\", text)  # Remove references section\n",
        "    text = re.sub(r\"(Figure|Table) \\d+.*\", \"\", text)  # Remove figure/table captions\n",
        "    return text\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts and preprocesses text from a given PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return preprocess_text(text.strip())\n",
        "\n",
        "# Step 3: Text Chunking Function (Handle Large Texts)\n",
        "def chunk_text(text, chunk_size=512):\n",
        "    \"\"\"Split text into chunks of a given token size.\"\"\"\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)] # Corrected the range function call\n",
        "\n",
        "def create_embedding(text):\n",
        "    \"\"\"Generate embedding for the given text, handling chunking and truncation.\"\"\"\n",
        "    chunks = chunk_text(text, chunk_size=512)\n",
        "    chunk_vectors = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        tokens = tokenizer(chunk, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            embeddings = embedding_model(**tokens).last_hidden_state.mean(dim=1).detach().numpy()\n",
        "        chunk_vectors.append(embeddings)\n",
        "\n",
        "    return np.mean(chunk_vectors, axis=0).reshape(1, -1)\n",
        "\n",
        "# Step 4: Custom VectorStore Implementation with FAISS for Similarity Search\n",
        "class SimpleVectorStore:\n",
        "    def __init__(self, dimension=768):\n",
        "        self.vectors = []\n",
        "        self.metadata = []\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "    def add_vector(self, key, vector, metadata=None):\n",
        "        if len(vector.shape) == 1:\n",
        "            vector = vector.reshape(1, -1)\n",
        "        self.vectors.append({\"key\": key, \"vector\": vector, \"metadata\": metadata})\n",
        "        self.index.add(vector.astype(np.float32))\n",
        "\n",
        "    def search(self, query_vector=None, key=None, top_k=10):\n",
        "        if key:\n",
        "            return [v for v in self.vectors if v[\"key\"] == key]\n",
        "        elif query_vector is not None:\n",
        "            query_vector = query_vector.reshape(1, -1) if query_vector.ndim == 1 else query_vector\n",
        "            distances, indices = self.index.search(query_vector.astype(np.float32), top_k)\n",
        "            results = [\n",
        "                {\"key\": self.vectors[idx][\"key\"], \"score\": 1 / (1 + distances[0][i]), \"metadata\": self.vectors[idx][\"metadata\"]}\n",
        "                for i, idx in enumerate(indices[0])\n",
        "            ]\n",
        "            return results\n",
        "        return []\n",
        "\n",
        "vector_store = SimpleVectorStore()\n",
        "\n",
        "# Step 5: Store Conference Papers in the VectorStore\n",
        "def store_conference_papers(conference_papers):\n",
        "    for conference, pdf_paths in conference_papers.items():\n",
        "        for pdf_path in pdf_paths:\n",
        "            paper_text = extract_text_from_pdf(pdf_path)\n",
        "            vector = create_embedding(paper_text)\n",
        "            vector_store.add_vector(key=conference, vector=vector, metadata={\"text\": paper_text})\n",
        "    print(\"All conference papers have been stored in the VectorStore.\")\n",
        "\n",
        "# Step 6: Define Conference Keywords\n",
        "conference_keywords = {\n",
        "    \"CVPR\": [\"object detection\", \"image segmentation\", \"computer vision tasks\", \"convolutional networks\"],\n",
        "    \"EMNLP\": [\"language models\", \"semantic parsing\", \"text classification\", \"token embeddings\"],\n",
        "    \"KDD\": [\"data clustering\", \"knowledge discovery\", \"graph mining\", \"recommendation systems\"],\n",
        "    \"NeurIPS\": [\"stochastic gradient descent\", \"adversarial training\", \"multi-agent systems\", \"gradient stability\"],\n",
        "    \"TMLR\": [\"optimization techniques\", \"mathematical proofs\", \"theoretical guarantees\", \"learning rates\"]\n",
        "}\n",
        "\n",
        "def compute_keyword_overlap(text, conference):\n",
        "    keywords = conference_keywords[conference]\n",
        "    overlap = sum(1 for word in keywords if word in text.lower())\n",
        "    return overlap\n",
        "\n",
        "# Step 7: Match a New Paper to a Conference\n",
        "def match_to_conference(new_pdf_path):\n",
        "    new_text = extract_text_from_pdf(new_pdf_path)\n",
        "    new_vector = create_embedding(new_text)\n",
        "\n",
        "    results = vector_store.search(query_vector=new_vector, top_k=10)\n",
        "\n",
        "    similarity_sums = defaultdict(float)\n",
        "    for result in results:\n",
        "        conference = result['key']\n",
        "        similarity = result['score']\n",
        "        similarity_sums[conference] += similarity\n",
        "\n",
        "    for conference in similarity_sums:\n",
        "        overlap_score = compute_keyword_overlap(new_text, conference)\n",
        "        similarity_sums[conference] += 0.5 * overlap_score  # Increased weight for keyword overlap\n",
        "\n",
        "    for conference in similarity_sums:\n",
        "        if conference == \"NeurIPS\":\n",
        "            similarity_sums[conference] += 0.3  # Small positive bias for NeurIPS\n",
        "\n",
        "    sorted_conferences = sorted(similarity_sums.items(), key=lambda x: x[1], reverse=True)\n",
        "    best_conference, best_score = sorted_conferences[0]\n",
        "\n",
        "    if len(sorted_conferences) > 1:\n",
        "        second_conference, second_score = sorted_conferences[1]\n",
        "        if best_conference == \"EMNLP\" and second_conference == \"NeurIPS\" and (best_score - second_score) < 0.1:\n",
        "            best_conference, best_score = second_conference, second_score\n",
        "\n",
        "    rationale = generate_rationale(best_conference, new_text)\n",
        "    return best_conference, best_score, rationale\n",
        "\n",
        "# Step 8: Generate Rationale for the Matched Conference\n",
        "def generate_rationale(conference, new_paper_text):\n",
        "    conference_papers = []\n",
        "    for result in vector_store.search(query_vector=None, key=conference, top_k=10):\n",
        "        conference_papers.append(result['metadata']['text'])\n",
        "\n",
        "    conference_text = \" \".join(conference_papers)\n",
        "    input_text = f\"The new paper's content is as follows: {new_paper_text[:500]}... The conference {conference} focuses on {conference_text[:500]}...\"\n",
        "\n",
        "    rationale = text_generation_model(input_text, max_new_tokens=200, num_return_sequences=1)\n",
        "    return rationale[0]['generated_text']\n",
        "\n",
        "# Step 9: Define File Paths for Conference Papers\n",
        "conference_papers = {\n",
        "    \"CVPR\": [\"/content/R006.pdf\", \"/content/R007.pdf\", \"/content/cvpr7.pdf\", \"/content/cvpr6.pdf\", \"/content/cvpr5.pdf\"],\n",
        "    \"EMNLP\": [\"/content/R008.pdf\", \"/content/R009.pdf\", \"/content/emnlp5.pdf\", \"/content/emnlp6.pdf\", \"/content/emnlp7.pdf\"],\n",
        "    \"KDD\": [\"/content/R010.pdf\", \"/content/R011.pdf\", \"/content/kdd6.pdf\", \"/content/kdd7.pdf\", \"/content/kdd5.pdf\"],\n",
        "    \"NeurIPS\": [\"/content/R012.pdf\", \"/content/R013.pdf\", \"/content/neurlps7.pdf\", \"/content/neurlps5.pdf\", \"/content/neurlps6.pdf\"],\n",
        "    \"TMLR\": [\"/content/R014.pdf\", \"/content/R015.pdf\", \"/content/tmlr7.pdf\", \"/content/tmlr5.pdf\", \"/content/tmlr6.pdf\"]\n",
        "}\n",
        "\n",
        "# Step 10: Store the Papers in the VectorStore\n",
        "store_conference_papers(conference_papers)\n",
        "\n",
        "# Step 11: Match a New Paper to a Conference and Generate Rationale\n",
        "new_paper_path = \"/content/P010.pdf\"\n",
        "matched_conference, total_score, rationale = match_to_conference(new_paper_path)\n",
        "\n",
        "print(f\"The new paper is most similar to Conference {matched_conference} with a total similarity score of {total_score:.2f}\")\n",
        "print(f\"Rationale for matching the paper to Conference {matched_conference}: {rationale}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7hn8CcfpfN5",
        "outputId": "363c6574-11fe-40a1-855c-7ae4e1ab559c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All conference papers have been stored in the VectorStore.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new paper is most similar to Conference NeurIPS with a total similarity score of 0.36\n",
            "Rationale for matching the paper to Conference NeurIPS: The new paper's content is as follows: Enhanced Reinforcement Learning for Recommender Systems: Maximizing Sample Efficiency and Minimizing Variance Abstract Optimizing long-term user satisfaction in recommender systems, such as news feeds, is crucial during continuous user-system interactions. Reinforcement learning has shown promise in addressing this challenge. However, practical hurdles like low sample efficiency, potential risks, and high variance hinder the implementation of deep reinforcement learning in online systems. We int... The conference NeurIPS focuses on Safe Predictors for Input-Output Specification Enforcement Abstract This paper presents an approach for designing neural networks, along with other machine learning models, which adhere to a collection of input-output specifica- tions. Our method involves the construction of a constrained predictor for each set of compatible constraints, and combining these predictors in a safe manner using a convex combination of their predictions. We demonstrate the applicability of this method with synthetic ... The paper's content is as follows: Enhanced Reinforcement Learning for Recommender Systems: Maximizing Sample Efficiency and Minimizing Variance Abstract Optimizing short-term user satisfaction in recommender systems, such as news feeds, is critical during continuous user-system interactions. Reinforcement learning has shown promise in addressing this challenge. However, practical hurdles like low sample efficiency, potential risks, and high variance hinder the implementation of deep reinforcement learning in online systems. We explain why our initial approach did not work in a paper based on the previous research. As with any form of training, the data are collected and analyzed in the lab. This allows us to generate more accurate predictions at the training level. Our approach... Our paper's content is as below: Reinforcement Learning for Recommender Systems: Maximizing Sample Efficiency and Minimizing Variance\n",
            "\n",
            "A User-System Theory for Evaluating Feedback by Predicting Input: Model-Based Optimization and Learning Abstract Assessing user experience using input-output\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMn7bD7tpt/2APuuybsuPn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}