{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBtiFDZyGOMFr47kxjizlK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DPravallika2005/SmartPaper/blob/main/Publishable_or_not.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuRHJR2y1s4Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import joblib\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import faiss\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Initialize Models and Utilities\n",
        "# Load pre-trained classifier and vectorizer for publishability check\n",
        "classifier = joblib.load(\"/content/drive/My Drive/research_paper_classifier.pkl\")\n",
        "vectorizer = joblib.load(\"/content/drive/My Drive/tfidf_vectorizer.pkl\")\n",
        "\n",
        "# Initialize the Embedding Model and Text Generation Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "embedding_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "text_generation_model = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "summarization_model = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\") # Initialize summarization model\n",
        "\n",
        "# Define Conference Keywords and Reference Papers\n",
        "conference_keywords = {\n",
        "    \"CVPR\": [\"object detection\", \"image segmentation\", \"computer vision tasks\", \"convolutional networks\"],\n",
        "    \"EMNLP\": [\"language models\", \"semantic parsing\", \"text classification\", \"token embeddings\"],\n",
        "    \"KDD\": [\"data clustering\", \"knowledge discovery\", \"graph mining\", \"recommendation systems\"],\n",
        "    \"NeurIPS\": [\"stochastic gradient descent\", \"adversarial training\", \"multi-agent systems\", \"gradient stability\"],\n",
        "    \"TMLR\": [\"optimization techniques\", \"mathematical proofs\", \"theoretical guarantees\", \"learning rates\"]\n",
        "}\n",
        "\n",
        "# Reference papers for each conference (provided by the user)\n",
        "conference_papers = {\n",
        "    \"CVPR\": [\"/content/R006.pdf\", \"/content/R007.pdf\", \"/content/cvpr7.pdf\", \"/content/cvpr6.pdf\", \"/content/cvpr5.pdf\"],\n",
        "    \"EMNLP\": [\"/content/R008.pdf\", \"/content/R009.pdf\", \"/content/emnlp5.pdf\", \"/content/emnlp6.pdf\", \"/content/emnlp7.pdf\"],\n",
        "    \"KDD\": [\"/content/R010.pdf\", \"/content/R011.pdf\", \"/content/kdd6.pdf\", \"/content/kdd7.pdf\", \"/content/kdd5.pdf\"],\n",
        "    \"NeurIPS\": [\"/content/R012.pdf\", \"/content/R013.pdf\", \"/content/neurlps7.pdf\", \"/content/neurlps5.pdf\", \"/content/neurlps6.pdf\"],\n",
        "    \"TMLR\": [\"/content/R014.pdf\", \"/content/R015.pdf\", \"/content/tmlr7.pdf\", \"/content/tmlr5.pdf\", \"/content/tmlr6.pdf\"]\n",
        "}\n",
        "\n",
        "# Custom VectorStore Implementation with FAISS\n",
        "class SimpleVectorStore:\n",
        "    def init(self, dimension=768):\n",
        "        self.vectors = []\n",
        "        self.metadata = []\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "    def add_vector(self, key, vector, metadata=None):\n",
        "        if len(vector.shape) == 1:\n",
        "            vector = vector.reshape(1, -1)\n",
        "        self.vectors.append({\"key\": key, \"vector\": vector, \"metadata\": metadata})\n",
        "        self.index.add(vector.astype(np.float32))\n",
        "\n",
        "    def search(self, query_vector=None, key=None, top_k=10):\n",
        "        if key:\n",
        "            return [v for v in self.vectors if v[\"key\"] == key]\n",
        "        elif query_vector is not None:\n",
        "            query_vector = query_vector.reshape(1, -1) if query_vector.ndim == 1 else query_vector\n",
        "            distances, indices = self.index.search(query_vector.astype(np.float32), top_k)\n",
        "            results = [\n",
        "                {\"key\": self.vectors[idx][\"key\"], \"score\": 1 / (1 + distances[0][i]), \"metadata\": self.vectors[idx][\"metadata\"]}\n",
        "                for i, idx in enumerate(indices[0])\n",
        "            ]\n",
        "            return results\n",
        "        return []\n",
        "\n",
        "vector_store = SimpleVectorStore()\n",
        "\n",
        "# Step 2: Function to Extract Text from PDFs\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and preprocesses the text extracted from a PDF.\"\"\"\n",
        "    text = \" \".join(text.split())  # Remove multiple spaces and newlines\n",
        "    text = re.sub(r\"(?i)References.*\", \"\", text)  # Remove references section\n",
        "    text = re.sub(r\"(Figure|Table) \\d+.*\", \"\", text)  # Remove figure/table captions\n",
        "    return text\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts and preprocesses text from a given PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return preprocess_text(text.strip())\n",
        "\n",
        "# Step 3: Function to Generate Embeddings\n",
        "def create_embedding(text):\n",
        "    \"\"\"Generate embedding for the given text, handling chunking and truncation.\"\"\"\n",
        "    chunks = text.split()  # Split the text into words (no chunking needed for this case)\n",
        "    tokenized = tokenizer(\" \".join(chunks), truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        embedding = embedding_model(**tokenized).last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    return embedding\n",
        "\n",
        "# Step 4: Conference Matching Function\n",
        "def compute_keyword_overlap(text, conference):\n",
        "    keywords = conference_keywords[conference]\n",
        "    overlap = sum(1 for word in keywords if word in text.lower())\n",
        "    return overlap\n",
        "\n",
        "def generate_rationale(conference, new_paper_text):\n",
        "    \"\"\"\n",
        "    Generate a rationale as a single, coherent 100-word paragraph for matching a paper to a conference.\n",
        "    \"\"\"\n",
        "    # Combine content from reference papers of the matched conference\n",
        "    conference_papers_list = [\n",
        "        extract_text_from_pdf(paper) for paper in conference_papers[conference]\n",
        "    ]\n",
        "    conference_text = \" \".join(conference_papers_list)\n",
        "\n",
        "    # Input for rationale generation\n",
        "    input_text = (\n",
        "        f\"The proposed paper introduces key concepts that align with the topics emphasized by {conference}. \"\n",
        "        f\"It highlights: {new_paper_text[:300]}... \"\n",
        "        f\"The conference focuses on areas like: {conference_text[:300]}. \"\n",
        "        f\"Generate a coherent, formal, and concise 100-word paragraph explaining why this paper is suitable for the conference.\"\n",
        "    )\n",
        "\n",
        "    # Generate rationale using the summarization model\n",
        "    rationale_output = summarization_model(\n",
        "        input_text, max_length=100, min_length=100, do_sample=False\n",
        "    )\n",
        "    # Return the generated paragraph\n",
        "    return rationale_output[0]['summary_text']\n",
        "\n",
        "# Update the match_to_conference function to use the new rationale generation method\n",
        "def match_to_conference(new_pdf_path):\n",
        "    new_text = extract_text_from_pdf(new_pdf_path)\n",
        "    new_vector = create_embedding(new_text)\n",
        "\n",
        "    results = vector_store.search(query_vector=new_vector, top_k=10)\n",
        "\n",
        "    similarity_sums = defaultdict(float)\n",
        "    for result in results:\n",
        "        conference = result['key']\n",
        "        similarity = result['score']\n",
        "        similarity_sums[conference] += similarity\n",
        "\n",
        "    for conference in similarity_sums:\n",
        "        overlap_score = compute_keyword_overlap(new_text, conference)\n",
        "        similarity_sums[conference] += 0.5 * overlap_score\n",
        "\n",
        "    sorted_conferences = sorted(similarity_sums.items(), key=lambda x: x[1], reverse=True)\n",
        "    best_conference, best_score = sorted_conferences[0]\n",
        "\n",
        "    # Generate rationale with the new function\n",
        "    rationale = generate_rationale(best_conference, new_text)\n",
        "    return best_conference, best_score, rationale\n",
        "\n",
        "# Update the CSV generation loop to include the new rationale\n",
        "def classify_and_match_papers(input_folder):\n",
        "    rows = []\n",
        "    for conference, papers in conference_papers.items():\n",
        "        for paper_path in papers:\n",
        "            paper_text = extract_text_from_pdf(paper_path)\n",
        "            paper_embedding = create_embedding(paper_text)\n",
        "            vector_store.add_vector(conference, paper_embedding, metadata={\"path\": paper_path})\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            paper_id = filename.split(\".\")[0]\n",
        "            pdf_path = os.path.join(input_folder, filename)\n",
        "\n",
        "            # Extract text and predict publishability\n",
        "            pdf_text = extract_text_from_pdf(pdf_path)\n",
        "            pdf_features = vectorizer.transform([pdf_text])\n",
        "            prediction = classifier.predict(pdf_features)\n",
        "\n",
        "            if prediction == 1:\n",
        "                # Publishable paper: match to a conference and generate rationale\n",
        "                conference, score, rationale = match_to_conference(pdf_path)\n",
        "                rows.append([paper_id, 1, conference, rationale])\n",
        "            else:\n",
        "                # Non-publishable paper: label as 'NA'\n",
        "                rows.append([paper_id, 0, \"NA\", \"NA\"])\n",
        "\n",
        "    # Save results to CSV\n",
        "    with open(\"/content/drive/My Drive/research_paper_classification.csv\", mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Paper ID\", \"Publishable\", \"Conference\", \"Rationale\"])\n",
        "        writer.writerows(rows)\n",
        "\n",
        "\n",
        "# Step 7: Run the Process\n",
        "input_folder = \"/content/drive/My Drive/Test_Papers\"\n",
        "classify_and_match_papers(input_folder)\n",
        "\n",
        "print(\"CSV file with results has been saved.\")"
      ]
    }
  ]
}